# -*- coding: utf-8 -*-
"""main_code_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-qivjqJ2BV3aV9psK-7hRJr3SdrrXsXt
"""

!pip install dopamine-rl==3.1.9

import numpy as np
import os
import sys

import dopamine
from dopamine.discrete_domains import run_experiment
from dopamine.colab import utils as colab_utils
from absl import flags
import gin.tf
import matplotlib

from google.colab import drive 
drive.mount('/content/drive')
path = '/content/drive/My Drive/SaveFiles/Data/Dopamine_github/ExperimentsSeeds/Experiments/'
sys.path.append(path)


from dqn_agent_new import *
from rainbow_agent_new import *
from quantile_agent_new import *
from implicit_quantile_agent_new import *

agents = {
    'dqn': JaxDQNAgentNew,
    'rainbow': JaxRainbowAgentNew,
    'quantile': JaxQuantileAgentNew,
    'implicit': JaxImplicitQuantileAgentNew,
}

inits = {'xavier':{'scale':1.0,'mode':'fan_avg','distribution':'uniform'},
         'var_sca_ori':{'scale':1.0/jnp.sqrt(3.0),'mode':'fan_in','distribution':'uniform'},
         'var_sca_0_1':{'scale':0.1,'mode':'fan_in','distribution':'uniform'}, 
         'var_sca_0_3':{'scale':0.3,'mode':'fan_in','distribution':'uniform'}, 
         'var_sca_0_8':{'scale':0.8,'mode':'fan_in','distribution':'uniform'}, 
         'var_sca_1':{'scale':1,'mode':'fan_in','distribution':'uniform'}, 
         'var_sca_2':{'scale':2,'mode':'fan_in','distribution':'uniform'}, 
         'var_sca_10':{'scale':10,'mode':'fan_in','distribution':'uniform'}}

num_runs = 1
environments = ['cartpole', 'acrobot']
seeds = [True, False]

for seed in seeds:
  for agent in agents:
    for env in environments:
      for init in inits:
        for i in range (1, num_runs + 1):
          print({agents[agent]})
          print(f'{agents[agent]}')

          LOG_PATH = os.path.join(f'{path}{seed}{i}_{agent}_{env}_{init}', f'dqn_test{i}')
          sys.path.append(path) 
          
          def create_agent(sess, environment, summary_writer=None):
            return agents[agent](num_actions=environment.action_space.n)
      
          gin_file = f'{path}{agent}_{env}.gin'

          '''
          gin_bindings = [f"{agents[agent]}.initzer = variance_scaling_1",
                          f"{agents[agent]}.initzer = variance_scaling_2"] '''
          
          #gin_bindings = f'{agents[agent]}.initzer,[variance_scaling_1, variance_scaling_2]

          agent_name = agents[agent].__name__
          s = '"'+inits[init]['mode']+'"' 
          t = '"'+inits[init]['distribution']+'"'

          gin_bindings = [f"{agent_name}.seed=None"] if seed is False else [f"{agent_name}.seed={i}",
                          f"{agent_name}.scale = {inits[init]['scale']}",
                          f"{agent_name}.mode = {s}",
                          f"{agent_name}.distribution = {t}"]    

          gin.parse_config_files_and_bindings([gin_file], gin_bindings, skip_unknown=False)
          agent_runner = run_experiment.TrainRunner(LOG_PATH, create_agent)

          print(f'Will train agent {agent} in {env}, run {i}, please be patient, may be a while...')
          agent_runner.run_experiment()
          print('Done training!')